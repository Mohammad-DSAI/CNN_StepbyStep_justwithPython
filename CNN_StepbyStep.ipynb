{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**By the end of this notebook, you'll be able to:**\n",
        "\n",
        "1Explain the convolution operation\n",
        "\n",
        "-Apply two different types of pooling operation\n",
        "\n",
        "-Identify the components used in a convolutional neural network (padding,\n",
        "stride, filter, ...) and their purpose\n",
        "\n",
        "-Build a convolutional neural network"
      ],
      "metadata": {
        "id": "6Dz3LU25zO6Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gHc6tF0ApCpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from public_tests import *\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "#load_ext autoreload\n",
        "#autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outline**\n",
        "\n",
        "**Convolution functions, including:**\n",
        "\n",
        "-Zero Padding\n",
        "\n",
        "-Convolve window\n",
        "\n",
        "-Convolution forward\n",
        "\n",
        "-Convolution backward (optional)\n",
        "\n",
        "**Pooling functions, including:**\n",
        "\n",
        "-Pooling forward\n",
        "\n",
        "-Create mask\n",
        "\n",
        "-Distribute value\n",
        "\n",
        "-Pooling backward (optional)\n",
        "\n",
        "**Convolutional Neural Networks**"
      ],
      "metadata": {
        "id": "RIIKDKSf0U8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: zero_pad\n",
        "\n",
        "def zero_pad(X, pad):\n",
        "    \"\"\"\n",
        "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image,\n",
        "    as illustrated in Figure 1.\n",
        "\n",
        "    Argument:\n",
        "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
        "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
        "\n",
        "    Returns:\n",
        "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "    #(≈ 1 line)\n",
        "    # X_pad = None\n",
        "    # YOUR CODE STARTS HERE\n",
        "    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)))\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    return X_pad"
      ],
      "metadata": {
        "id": "LnEhiU5C16W9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(4, 3, 3, 2)\n",
        "x_pad = zero_pad(x, 3)\n",
        "print (\"x.shape =\\n\", x.shape)\n",
        "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
        "print (\"x[1,1] =\\n\", x[1, 1])\n",
        "print (\"x_pad[1,1] =\\n\", x_pad[1, 1])\n",
        "\n",
        "assert type(x_pad) == np.ndarray, \"Output must be a np array\"\n",
        "assert x_pad.shape == (4, 9, 9, 2), f\"Wrong shape: {x_pad.shape} != (4, 9, 9, 2)\"\n",
        "print(x_pad[0, 0:2,:, 0])\n",
        "assert np.allclose(x_pad[0, 0:2,:, 0], [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 1e-15), \"Rows are not padded with zeros\"\n",
        "assert np.allclose(x_pad[0, :, 7:9, 1].transpose(), [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 1e-15), \"Columns are not padded with zeros\"\n",
        "assert np.allclose(x_pad[:, 3:6, 3:6, :], x, 1e-15), \"Internal values are different\"\n",
        "\n",
        "fig, axarr = plt.subplots(1, 2)\n",
        "axarr[0].set_title('x')\n",
        "axarr[0].imshow(x[0, :, :, 0])\n",
        "axarr[1].set_title('x_pad')\n",
        "axarr[1].imshow(x_pad[0, :, :, 0])\n",
        "zero_pad_test(zero_pad)"
      ],
      "metadata": {
        "id": "Z58C6o1a6m1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single Step of Convolution**\n",
        "\n",
        "apply the filter to a single position of the input. This will be used to build a convolutional unit, which:\n",
        "\n",
        "-Takes an input volume\n",
        "\n",
        "-Applies a filter at every position of the input\n",
        "\n",
        "-Outputs another volume (usually of different size)"
      ],
      "metadata": {
        "id": "u9vmQFeo66tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conv_single_step**"
      ],
      "metadata": {
        "id": "stl0A64jBvMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: conv_single_step\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "    \"\"\"\n",
        "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation\n",
        "    of the previous layer.\n",
        "\n",
        "    Arguments:\n",
        "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
        "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
        "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
        "    \"\"\"\n",
        "\n",
        "    #(≈ 3 lines of code)\n",
        "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
        "    s = np.multiply(a_slice_prev,W)\n",
        "    # Sum over all entries of the volume s.\n",
        "    Z = np.sum(s)\n",
        "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
        "    b = np.squeeze(b)\n",
        "    Z = Z + b\n",
        "    # YOUR CODE STARTS HERE\n",
        "\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    return Z"
      ],
      "metadata": {
        "id": "HiI3qE6i63HK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "a_slice_prev = np.random.randn(4, 4, 3)\n",
        "W = np.random.randn(4, 4, 3)\n",
        "b = np.random.randn(1, 1, 1)\n",
        "\n",
        "Z = conv_single_step(a_slice_prev, W, b)\n",
        "print(\"Z =\", Z)\n",
        "conv_single_step_test(conv_single_step)\n",
        "\n",
        "assert (type(Z) == np.float64 or type(Z) == np.float32), \"You must cast the output to float\"\n",
        "assert np.isclose(Z, -6.999089450680221), \"Wrong value\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "133Y72_ACLk2",
        "outputId": "ca8a8e36-ea99-42a7-92da-b7f7a94ab95c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z = -6.999089450680221\n",
            "\u001b[92m All tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional Neural Networks - Forward Pass**"
      ],
      "metadata": {
        "id": "l-0LMGlcCRPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: conv_forward\n",
        "\n",
        "def conv_forward(A_prev, W, b, hparameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for a convolution function\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- output activations of the previous layer,\n",
        "        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
        "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
        "\n",
        "    Returns:\n",
        "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward() function\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve dimensions from A_prev's shape (≈1 line)\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "\n",
        "    # Retrieve dimensions from W's shape (≈1 line)\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "\n",
        "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "\n",
        "    # Compute the dimensions of the CONV output volume using the formula given above.\n",
        "    # Hint: use int() to apply the 'floor' operation. (≈2 lines)\n",
        "    n_H = int((n_H_prev + 2*pad - f)/stride) + 1\n",
        "    n_W = int((n_W_prev + 2*pad - f)/stride) + 1\n",
        "\n",
        "    # Initialize the output volume Z with zeros. (≈1 line)\n",
        "    Z = np.zeros((m, n_H, n_W, n_C))\n",
        "\n",
        "    # Create A_prev_pad by padding A_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "\n",
        "    for i in range(m):               # loop over the batch of training examples\n",
        "        a_prev_pad = A_prev_pad[i]          # Select ith training example's padded activation\n",
        "        for h in range(n_H):           # loop over vertical axis of the output volume\n",
        "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
        "            vert_start = stride * h\n",
        "            vert_end = vert_start  + f\n",
        "\n",
        "            for w in range(n_W):       # loop over horizontal axis of the output volume\n",
        "                # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
        "                horiz_start = stride * w\n",
        "                horiz_end = horiz_start + f\n",
        "\n",
        "                for c in range(n_C):   # loop over channels (= #filters) of the output volume\n",
        "\n",
        "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
        "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "\n",
        "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n",
        "                    weights = W[:, :, :, c]\n",
        "                    biases  = b[:, :, :, c]\n",
        "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
        "\n",
        "    # Save information in \"cache\" for the backprop\n",
        "    cache = (A_prev, W, b, hparameters)\n",
        "\n",
        "    return Z, cache\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(2, 5, 7, 4)\n",
        "W = np.random.randn(3, 3, 4, 8)\n",
        "b = np.random.randn(1, 1, 1, 8)\n",
        "hparameters = {\"pad\" : 1,\n",
        "               \"stride\": 2}\n"
      ],
      "metadata": {
        "id": "KeDuPhuACTZO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
        "print(\"Z's mean =\\n\", np.mean(Z))\n",
        "print(\"Z[0,2,1] =\\n\", Z[0, 2, 1])\n",
        "print(\"cache_conv[0][1][2][3] =\\n\", cache_conv[0][1][2][3])\n",
        "conv_forward_test(conv_forward)"
      ],
      "metadata": {
        "id": "gd8dgoZEEWXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pool_forward**"
      ],
      "metadata": {
        "id": "9ET7TFcMGSfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: pool_forward\n",
        "\n",
        "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
        "    \"\"\"\n",
        "    Implements the forward pass of the pooling layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
        "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "\n",
        "    Returns:\n",
        "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve dimensions from the input shape\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "\n",
        "    # Retrieve hyperparameters from \"hparameters\"\n",
        "    f = hparameters[\"f\"]\n",
        "    stride = hparameters[\"stride\"]\n",
        "\n",
        "    # Define the dimensions of the output\n",
        "    n_H = int(1 + (n_H_prev - f) / stride)\n",
        "    n_W = int(1 + (n_W_prev - f) / stride)\n",
        "    n_C = n_C_prev\n",
        "\n",
        "    # Initialize output matrix A\n",
        "    A = np.zeros((m, n_H, n_W, n_C))\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    for i in range(m):                         # loop over the training examples\n",
        "        a_prev_slice = A_prev[i]\n",
        "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
        "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
        "            vert_start = stride * h\n",
        "            vert_end = vert_start + f\n",
        "\n",
        "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
        "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
        "                horiz_start = stride * w\n",
        "                horiz_end = horiz_start + f\n",
        "\n",
        "                for c in range (n_C):            # loop over the channels of the output volume\n",
        "\n",
        "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
        "                    a_slice_prev = a_prev_slice[vert_start:vert_end,horiz_start:horiz_end,c]\n",
        "\n",
        "                    # Compute the pooling operation on the slice.\n",
        "                    # Use an if statement to differentiate the modes.\n",
        "                    # Use np.max and np.mean.\n",
        "                    if mode == \"max\":\n",
        "                        A[i, h, w, c] = np.max(a_slice_prev)\n",
        "                    elif mode == \"average\":\n",
        "                        A[i, h, w, c] = np.mean(a_slice_prev)\n",
        "                    else:\n",
        "                        print(mode+ \"-type pooling layer NOT Defined\")\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
        "    cache = (A_prev, hparameters)\n",
        "\n",
        "    # Making sure your output shape is correct\n",
        "    assert(A.shape == (m, n_H, n_W, n_C))\n",
        "\n",
        "    return A, cache\n"
      ],
      "metadata": {
        "id": "CX0z7lShGVXW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 1: stride of 1\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(2, 5, 5, 3)\n",
        "hparameters = {\"stride\" : 1, \"f\": 3}\n",
        "\n",
        "A, cache = pool_forward(A_prev, hparameters, mode = \"max\")\n",
        "print(\"mode = max\")\n",
        "print(\"A.shape = \" + str(A.shape))\n",
        "print(\"A[1, 1] =\\n\", A[1, 1])\n",
        "print()\n",
        "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
        "print(\"mode = average\")\n",
        "print(\"A.shape = \" + str(A.shape))\n",
        "print(\"A[1, 1] =\\n\", A[1, 1])\n",
        "\n",
        "pool_forward_test(pool_forward)"
      ],
      "metadata": {
        "id": "s3BR-TLzGgON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Case 2: stride of 2\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(2, 5, 5, 3)\n",
        "hparameters = {\"stride\" : 2, \"f\": 3}\n",
        "\n",
        "A, cache = pool_forward(A_prev, hparameters)\n",
        "print(\"mode = max\")\n",
        "print(\"A.shape = \" + str(A.shape))\n",
        "print(\"A[0] =\\n\", A[0])\n",
        "print()\n",
        "\n",
        "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
        "print(\"mode = average\")\n",
        "print(\"A.shape = \" + str(A.shape))\n",
        "print(\"A[1] =\\n\", A[1])"
      ],
      "metadata": {
        "id": "lVgQLqwwGpvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What you should remember:**\n",
        "\n",
        "-A convolution extracts features from an input image by taking the dot product between the input data and a 2D array of weights (the filter).\n",
        "\n",
        "-The 2D output of the convolution is called the feature map\n",
        "\n",
        "-A convolution layer is where the filter slides over the image and computes the dot product.\n",
        "\n",
        "This transforms the input volume into an output volume of different size\n",
        "\n",
        "-Zero padding helps keep more information at the image borders, and is helpful for building deeper networks, because you can build a CONV layer without shrinking the height and width of the volumes\n",
        "\n",
        "-Pooling layers gradually reduce the height and width of the input by sliding a 2D window over each specified region, then summarizing the features in that region"
      ],
      "metadata": {
        "id": "yIbQHUXeG3R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**conv_backward**"
      ],
      "metadata": {
        "id": "Wtxr2aGlIB2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a convolution function\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
        "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
        "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
        "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
        "          numpy array of shape (f, f, n_C_prev, n_C)\n",
        "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
        "          numpy array of shape (1, 1, 1, n_C)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Retrieve information from \"cache\"\n",
        "    (A_prev, W, b, hparameters) = cache\n",
        "    # Retrieve dimensions from A_prev's shape\n",
        "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "    # Retrieve dimensions from W's shape\n",
        "    (f, f, n_C_prev, n_C) = W.shape\n",
        "\n",
        "    # Retrieve information from \"hparameters\"\n",
        "    stride = hparameters[\"stride\"]\n",
        "    pad = hparameters[\"pad\"]\n",
        "\n",
        "    # Retrieve dimensions from dZ's shape\n",
        "    (m, n_H, n_W, n_C) = dZ.shape\n",
        "\n",
        "    # Initialize dA_prev, dW, db with the correct shapes\n",
        "    dA_prev = np.zeros(A_prev.shape)\n",
        "    dW = np.zeros(W.shape)\n",
        "    db = np.zeros(b.shape) # b.shape = [1,1,1,n_C]\n",
        "\n",
        "    # Pad A_prev and dA_prev\n",
        "    A_prev_pad = zero_pad(A_prev, pad)\n",
        "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
        "\n",
        "    for i in range(m):                       # loop over the training examples\n",
        "\n",
        "        # select ith training example from A_prev_pad and dA_prev_pad\n",
        "        a_prev_pad = A_prev_pad[i]\n",
        "        da_prev_pad = dA_prev_pad[i]\n",
        "\n",
        "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
        "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
        "                for c in range(n_C):           # loop over the channels of the output volume\n",
        "\n",
        "                    # Find the corners of the current \"slice\"\n",
        "                    vert_start = stride * h\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = stride * w\n",
        "                    horiz_end = horiz_start + f\n",
        "\n",
        "                    # Use the corners to define the slice from a_prev_pad\n",
        "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
        "\n",
        "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
        "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
        "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
        "\n",
        "        # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
        "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
        "\n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "\n",
        "    return dA_prev, dW, db\n"
      ],
      "metadata": {
        "id": "4-nueUGXGsMl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
        "# which we'll use to test the conv_backward function\n",
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(10, 4, 4, 3)\n",
        "W = np.random.randn(2, 2, 3, 8)\n",
        "b = np.random.randn(1, 1, 1, 8)\n",
        "hparameters = {\"pad\" : 2,\n",
        "               \"stride\": 2}\n",
        "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
        "\n",
        "# Test conv_backward\n",
        "dA, dW, db = conv_backward(Z, cache_conv)\n",
        "\n",
        "print(\"dA_mean =\", np.mean(dA))\n",
        "print(\"dW_mean =\", np.mean(dW))\n",
        "print(\"db_mean =\", np.mean(db))\n",
        "\n",
        "assert type(dA) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert type(dW) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert type(db) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert dA.shape == (10, 4, 4, 3), f\"Wrong shape for dA  {dA.shape} != (10, 4, 4, 3)\"\n",
        "assert dW.shape == (2, 2, 3, 8), f\"Wrong shape for dW {dW.shape} != (2, 2, 3, 8)\"\n",
        "assert db.shape == (1, 1, 1, 8), f\"Wrong shape for db {db.shape} != (1, 1, 1, 8)\"\n",
        "assert np.isclose(np.mean(dA), 1.4524377), \"Wrong values for dA\"\n",
        "assert np.isclose(np.mean(dW), 1.7269914), \"Wrong values for dW\"\n",
        "assert np.isclose(np.mean(db), 7.8392325), \"Wrong values for db\"\n",
        "\n",
        "print(\"\\033[92m All tests passed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L6skStzIO54",
        "outputId": "cc76a0a2-f2e1-425b-d30e-00e575054ae7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dA_mean = 1.4524377775388075\n",
            "dW_mean = 1.7269914583139097\n",
            "db_mean = 7.839232564616838\n",
            "\u001b[92m All tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2 Pooling Layer - Backward Pass**\n",
        "\n",
        "**5.2.1 Max Pooling - Backward Pass**\n",
        "\n",
        "**Exercise 6 - create_mask_from_window**"
      ],
      "metadata": {
        "id": "ElKPZIdmIaEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask_from_window(x):\n",
        "    \"\"\"\n",
        "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
        "\n",
        "    Arguments:\n",
        "    x -- Array of shape (f, f)\n",
        "\n",
        "    Returns:\n",
        "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
        "    \"\"\"\n",
        "    # (≈1 line)\n",
        "    # mask = None\n",
        "    # YOUR CODE STARTS HERE\n",
        "    mask = (x == np.max(x))\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "    return mask"
      ],
      "metadata": {
        "id": "xQd3Qx5eIxs0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(2, 3)\n",
        "mask = create_mask_from_window(x)\n",
        "print('x = ', x)\n",
        "print(\"mask = \", mask)\n",
        "\n",
        "x = np.array([[-1, 2, 3],\n",
        "              [2, -3, 2],\n",
        "              [1, 5, -2]])\n",
        "\n",
        "y = np.array([[False, False, False],\n",
        "     [False, False, False],\n",
        "     [False, True, False]])\n",
        "mask = create_mask_from_window(x)\n",
        "\n",
        "assert type(mask) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert mask.shape == x.shape, \"Input and output shapes must match\"\n",
        "assert np.allclose(mask, y), \"Wrong output. The True value must be at position (2, 1)\"\n",
        "\n",
        "print(\"\\033[92m All tests passed.\")"
      ],
      "metadata": {
        "id": "FroDg9ZzIz9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2.2 - Average Pooling - Backward Pass**\n",
        "\n",
        "**Exercise 7 - distribute_value**"
      ],
      "metadata": {
        "id": "kwOGowsaI_fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distribute_value(dz, shape):\n",
        "    \"\"\"\n",
        "    Distributes the input value in the matrix of dimension shape\n",
        "\n",
        "    Arguments:\n",
        "    dz -- input scalar\n",
        "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
        "\n",
        "    Returns:\n",
        "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
        "    \"\"\"\n",
        "    # Retrieve dimensions from shape (≈1 line)\n",
        "    (n_H, n_W) = shape\n",
        "\n",
        "    # Compute the value to distribute on the matrix (≈1 line)\n",
        "    average = np.prod(shape)\n",
        "\n",
        "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
        "    a = (dz/average)*np.ones(shape)\n",
        "    # YOUR CODE STARTS HERE\n",
        "\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "    return a\n"
      ],
      "metadata": {
        "id": "KboF0LrxI4sg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = distribute_value(2, (2, 2))\n",
        "print('distributed value =', a)\n",
        "\n",
        "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert a.shape == (2, 2), f\"Wrong shape {a.shape} != (2, 2)\"\n",
        "assert np.sum(a) == 2, \"Values must sum to 2\"\n",
        "\n",
        "a = distribute_value(100, (10, 10))\n",
        "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
        "assert a.shape == (10, 10), f\"Wrong shape {a.shape} != (10, 10)\"\n",
        "assert np.sum(a) == 100, \"Values must sum to 100\"\n",
        "\n",
        "print(\"\\033[92m All tests passed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1V1LESCMAC5",
        "outputId": "f8c76241-4423-4b66-ea9c-82c1351895b3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distributed value = [[0.5 0.5]\n",
            " [0.5 0.5]]\n",
            "\u001b[92m All tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2.3 Putting it Together: Pooling Backward**\n",
        "\n",
        "**Exercise 8 - pool_backward**"
      ],
      "metadata": {
        "id": "EFbQ_KT7MJZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pool_backward(dA, cache, mode = \"max\"):\n",
        "    \"\"\"\n",
        "    Implements the backward pass of the pooling layer\n",
        "\n",
        "    Arguments:\n",
        "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
        "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters\n",
        "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
        "    \"\"\"\n",
        "    # Retrieve information from cache (≈1 line)\n",
        "    (A_prev, hparameters) = cache\n",
        "\n",
        "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
        "    stride = hparameters[\"stride\"]\n",
        "    f = hparameters[\"f\"]\n",
        "\n",
        "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
        "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "    m, n_H, n_W, n_C = dA.shape\n",
        "\n",
        "    # Initialize dA_prev with zeros (≈1 line)\n",
        "    dA_prev = np.zeros(A_prev.shape)\n",
        "\n",
        "    for i in range(m): # loop over the training examples\n",
        "\n",
        "        # select training example from A_prev (≈1 line)\n",
        "        a_prev = A_prev[i,:,:,:]\n",
        "\n",
        "        for h in range(n_H):                   # loop on the vertical axis\n",
        "            for w in range(n_W):               # loop on the horizontal axis\n",
        "                for c in range(n_C):           # loop over the channels (depth)\n",
        "\n",
        "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
        "                    vert_start  = h * stride\n",
        "                    vert_end    = h * stride + f\n",
        "                    horiz_start = w * stride\n",
        "                    horiz_end   = w * stride + f\n",
        "\n",
        "                    # Compute the backward propagation in both modes.\n",
        "                    if mode == \"max\":\n",
        "\n",
        "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
        "                        a_prev_slice = a_prev[ vert_start:vert_end, horiz_start:horiz_end, c ]\n",
        "\n",
        "                        # Create the mask from a_prev_slice (≈1 line)\n",
        "                        mask = create_mask_from_window( a_prev_slice )\n",
        "\n",
        "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
        "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
        "\n",
        "                    elif mode == \"average\":\n",
        "\n",
        "                        # Get the value da from dA (≈2 line)\n",
        "                        da = dA[i, h, w, c]\n",
        "\n",
        "                        # Define the shape of the filter as fxf (≈1 line)\n",
        "                        shape = (f,f)\n",
        "\n",
        "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
        "\n",
        "    # Making sure your output shape is correct\n",
        "    assert(dA_prev.shape == A_prev.shape)\n",
        "\n",
        "    return dA_prev\n"
      ],
      "metadata": {
        "id": "2cfXE3AoMRYM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "A_prev = np.random.randn(5, 5, 3, 2)\n",
        "hparameters = {\"stride\" : 1, \"f\": 2}\n",
        "A, cache = pool_forward(A_prev, hparameters)\n",
        "print(A.shape)\n",
        "print(cache[0].shape)\n",
        "dA = np.random.randn(5, 4, 2, 2)\n",
        "\n",
        "dA_prev1 = pool_backward(dA, cache, mode = \"max\")\n",
        "print(\"mode = max\")\n",
        "print('mean of dA = ', np.mean(dA))\n",
        "print('dA_prev1[1,1] = ', dA_prev1[1, 1])\n",
        "print()\n",
        "dA_prev2 = pool_backward(dA, cache, mode = \"average\")\n",
        "print(\"mode = average\")\n",
        "print('mean of dA = ', np.mean(dA))\n",
        "print('dA_prev2[1,1] = ', dA_prev2[1, 1])\n",
        "\n",
        "assert type(dA_prev1) == np.ndarray, \"Wrong type\"\n",
        "assert dA_prev1.shape == (5, 5, 3, 2), f\"Wrong shape {dA_prev1.shape} != (5, 5, 3, 2)\"\n",
        "assert np.allclose(dA_prev1[1, 1], [[0, 0],\n",
        "                                    [ 5.05844394, -1.68282702],\n",
        "                                    [ 0, 0]]), \"Wrong values for mode max\"\n",
        "assert np.allclose(dA_prev2[1, 1], [[0.08485462,  0.2787552],\n",
        "                                    [1.26461098, -0.25749373],\n",
        "                                    [1.17975636, -0.53624893]]), \"Wrong values for mode average\"\n",
        "print(\"\\033[92m All tests passed.\")"
      ],
      "metadata": {
        "id": "30pKwZoqMaKD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}